{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\vic\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.39.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\vic\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\vic\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vic\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vic\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vic\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vic\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\vic\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\vic\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\vic\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\vic\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\vic\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vic\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vic\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vic\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vic\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vic\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vic\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_short, stem_text\n",
    "import pickle\n",
    "import en_core_web_sm\n",
    "from transformers import BertTokenizer \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of those\n",
    "\n",
    "pandas: Used for data manipulation and analysis.\n",
    "\n",
    "re: Provides support for regular expressions for string manipulation.\n",
    "\n",
    "gensim: Popular library for text processing and analysis.\n",
    "\n",
    "pickle: Used for serializing and deserializing Python objects.\n",
    "\n",
    "en_core_web_sm: SpaCy language model for English text processing.\n",
    "\n",
    "transformers.BertTokenizer: Part of the Hugging Face Transformers library, used for tokenization with BERT models.\n",
    "\n",
    "nltk: Natural Language Toolkit, used for text processing tasks.\n",
    "\n",
    "nltk.corpus.stopwords: Stopwords corpus from NLTK, used for removing common words in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json\"\n",
    "df = pd.read_json(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  target  \\\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "\n",
       "            target_names  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of data does it contain?\n",
    "\n",
    "Text contains text data related to newsgroup posts. Each row represents a newsgroup post ('content'), the target category('target') and the corresponding target name('target_names')\n",
    "\n",
    "How many entries does it contain? --> 11314 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 11314 entries, 0 to 11313\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   content       11314 non-null  object\n",
      " 1   target        11314 non-null  int64 \n",
      " 2   target_names  11314 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 353.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below here are listed all target names & their distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_names\n",
      "rec.sport.hockey            600\n",
      "soc.religion.christian      599\n",
      "rec.motorcycles             598\n",
      "rec.sport.baseball          597\n",
      "sci.crypt                   595\n",
      "rec.autos                   594\n",
      "sci.med                     594\n",
      "comp.windows.x              593\n",
      "sci.space                   593\n",
      "comp.os.ms-windows.misc     591\n",
      "sci.electronics             591\n",
      "comp.sys.ibm.pc.hardware    590\n",
      "misc.forsale                585\n",
      "comp.graphics               584\n",
      "comp.sys.mac.hardware       578\n",
      "talk.politics.mideast       564\n",
      "talk.politics.guns          546\n",
      "alt.atheism                 480\n",
      "talk.politics.misc          465\n",
      "talk.religion.misc          377\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['target_names'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df['content'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first contents value matches the target name rec.autos because the content is about a car\n",
    "\n",
    "Which business question can this dataset address?\n",
    "\n",
    "It can help us understand discussions, trends and opinions related to the topics covered in the newsgroups, which can then be used for market research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text): # Here we define the function and specify that it takes in a single argument, text\n",
    "    # Define the regex pattern to match the unwanted lines and words\n",
    "    pattern = r'^(From:|Article-I.D.:|Organization:|Lines:|NNTP-Posting-Host:|Distribution:|Reply-To:|XNewsreader:|Expires:|\\s+-{1,}|Subject:|Summary:|Keywords:).*$'\n",
    "    #Here, we define a regular expression pattern (pattern) using r'', which matches lines starting with specific strings (e.g., 'From:', 'Subject:')\n",
    "    # and unwanted characters (e.g., multiple hyphens preceded by a space). The ^ and $ symbols ensure that the entire line is matched.\n",
    "    # Use re.sub() to remove unwanted lines and words\n",
    "    processed_text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    #We use re.sub() to substitute the matched pattern with an empty string '' in the text. The flags argument with re.IGNORECASE makes\n",
    "    # the matching case-insensitive, and re.MULTILINE allows matching across multiple lines.\n",
    "    return processed_text.strip()\n",
    "# Here the processed text is returned after stripping leading trailing whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n"
     ]
    }
   ],
   "source": [
    "# Here we apply the \"preprocess_text\" function to each entry in the 'content' column and store the result in a new column called \"data\"\n",
    "df['data'] = df['content'].apply(preprocess_text)\n",
    "\n",
    "# Display the first entry after preprocessing\n",
    "print(df['data'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are we doing this? --> Part B\n",
    "\n",
    "We are performing this preprocessing to clean the text data by removing metadata lines (starting with specific patterns) and irrelevant words. This step helps focus the analysis on the actual content of the newsgroup posts rather than including irrelevant or repetitive information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was wondering if anyone out there could enlighten me on this car I saw the other day It was a door sports car looked to be from the late s early s It was called a Bricklin The doors were really small In addition the front bumper was separate from the rest of the body This is all I know If anyone can tellme a model name engine specs years of production where this car is made history or whatever info you have on this funky looking car please e mail Thanks IL\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing functions to the 'data' column\n",
    "df['data'] = df['data'].apply(lambda x: strip_numeric(x)) # Remove numbers\n",
    "df['data'] = df['data'].apply(lambda x: strip_punctuation(x)) # Remove punctuation\n",
    "df['data'] = df['data'].apply(lambda x: strip_multiple_whitespaces(x)) # Remove multiple whitespaces\n",
    "\n",
    "# Display the first entry after preprocessing\n",
    "print(df['data'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part C\n",
    "\n",
    "We removed, numeric digits, punctuation and multiple whitespaces to prepare the text data in a cleaner format that is more suitable for natural language processing or machine learning tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was wondering if anyone out there could enlighten me on this car i saw the other day it was a door sports car looked to be from the late s early s it was called a bricklin the doors were really small in addition the front bumper was separate from the rest of the body this is all i know if anyone can tellme a model name engine specs years of production where this car is made history or whatever info you have on this funky looking car please e mail thanks il\n"
     ]
    }
   ],
   "source": [
    "# Transform all letters in 'data' to lowercase\n",
    "df['data'] = df['data'].apply(lambda x: x.lower())\n",
    "\n",
    "# Display the first entry after transforming to lowercase\n",
    "print(df['data'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim Stopwords (sorted):\n",
      "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'computer', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'did', 'didn', 'do', 'does', 'doesn', 'doing', 'don', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself', 'just', 'keep', 'kg', 'km', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'make', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'quite', 'rather', 're', 'really', 'regarding', 'same', 'say', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'unless', 'until', 'up', 'upon', 'us', 'used', 'using', 'various', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves']\n",
      "\n",
      "NLTK Stopwords (sorted):\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "# Print and compare the stopwords\n",
    "print(\"Gensim Stopwords (sorted):\")\n",
    "print(sorted(STOPWORDS))\n",
    "\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "print(\"\\nNLTK Stopwords (sorted):\")\n",
    "print(sorted(nltk_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'not' is included in NLTK stopwords.\n",
      "\n",
      "'no' is included in NLTK stopwords.\n",
      "\n",
      "'nor' is included in NLTK stopwords.\n",
      "\n",
      "'neither' is not included in NLTK stopwords.\n",
      "\n",
      "'never' is not included in NLTK stopwords.\n",
      "\n",
      "'none' is not included in NLTK stopwords.\n",
      "\n",
      "'cannot' is not included in NLTK stopwords.\n",
      "\n",
      "'could not' is not included in NLTK stopwords.\n",
      "\n",
      "'would not' is not included in NLTK stopwords.\n",
      "\n",
      "'should not' is not included in NLTK stopwords.\n"
     ]
    }
   ],
   "source": [
    "# Check if negations are included in the nltk stopwords\n",
    "negations = ['not', 'no', 'nor', 'neither', 'never', 'none', 'cannot', 'could not', 'would not', 'should not']\n",
    "for neg in negations:\n",
    "    if neg in nltk_stopwords:\n",
    "        print(f\"\\n'{neg}' is included in NLTK stopwords.\")\n",
    "    else:\n",
    "        print(f\"\\n'{neg}' is not included in NLTK stopwords.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part D -- Is it reasonable to include negotiations in stopwords?\n",
    "\n",
    "Including negations in stopwords is reasonable because they often do not provide semantic meaning in many text analysis tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adjusted NLTK Stopwords (sorted):\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'more', 'most', 'my', 'myself', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', 'she', \"she's\", 'should', \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'we', 'were', 'weren', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "# Stopwords to be removed\n",
    "stopwords_to_remove = [\n",
    "    \"aren't\", \"isn\", \"isn't\", \"mightn\", \"mightn't\", \"mustn\", \"mustn't\",\n",
    "    \"needn\", \"needn't\", \"no\", \"nor\", \"not\", \"shan't\", \"shouldn\", \"shouldn't\",\n",
    "    \"wasn\", \"wasn't\", \"weren't\", \"wouldn\", \"wouldn't\"\n",
    "]\n",
    "\n",
    "# Adjust nltk_stopwords list by removing specified stopwords\n",
    "for word in stopwords_to_remove:          #Here we iterate trough this list and use remove() method to remove each word from the nltk_stopwords list\n",
    "    if word in nltk_stopwords:\n",
    "        nltk_stopwords.remove(word)\n",
    "\n",
    "print(\"\\nAdjusted NLTK Stopwords (sorted):\")\n",
    "print(sorted(nltk_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wondering anyone could enlighten car saw day door sports car looked late early called bricklin doors really small addition front bumper separate rest body know anyone tellme model name engine specs years production car made history whatever info funky looking car please e mail thanks il\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords using gensim's remove_stopwords with nltk_stopwords\n",
    "df['data'] = df['data'].apply(lambda x: remove_stopwords(x, stopwords=nltk_stopwords))\n",
    "\n",
    "# Display the first entry after removing stopwords\n",
    "print(df['data'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wondering anyone could enlighten car saw day door sports car looked late early called bricklin doors really small addition front bumper separate rest body know anyone tellme model name engine specs years production car made history whatever info funky looking car please mail thanks\n"
     ]
    }
   ],
   "source": [
    "# Apply Strise_short to remove words with length less than 3\t\n",
    "df['data'] = df['data'].apply(lambda x: strip_short(x, minsize = 3))\n",
    "\n",
    "# Display the first entry after applying strip_short\n",
    "print(df['data'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part D 10 --> What does strip_short function do and why?\n",
    "\n",
    "It removes tokens (words pr parts of words) that are shorter tjan a specified length (here length=3) by using the parameter 'minsize'\n",
    "\n",
    "Purpose: Noise reduction, because short tokens like \"a\", \"an\", \"is\", \"it\", etc., often do not carry significant meaning in many natural language processing tasks. Removing them heps reducdd noise in the text data.\n",
    "\n",
    "Also by removing short tokens, the focus is shifted to longer tokens that may carry more semantic value in the cntext of your task\n",
    "\n",
    "Further more in many cases, removing short tokens can lead to better model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        wondering anyone could enlighten car saw day d...\n",
       "1        fair number brave souls upgraded clock oscilla...\n",
       "2        well folks mac plus finally gave ghost weekend...\n",
       "3        newsreader tin version robert kyanko rob rjck ...\n",
       "4        article cowcb world std com tombaker world std...\n",
       "                               ...                        \n",
       "11309    nyeda cnsvax uwec edu david nye neurology cons...\n",
       "11310    old mac mac plus problem screens blank sometim...\n",
       "11311    newsreader tin version installed cpu clone mot...\n",
       "11312    article qkgbuinnsn shelley washington edu bols...\n",
       "11313    stolen pasadena blue white honda cbrrr califor...\n",
       "Name: data, Length: 11314, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######\n",
    "df['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        wonder anyon could enlighten car saw day door ...\n",
      "1        fair number brave soul upgrad clock oscil shar...\n",
      "2        well folk mac plu final gave ghost weekend sta...\n",
      "3        newsread tin version robert kyanko rob rjck uu...\n",
      "4        articl cowcb world std com tombak world std co...\n",
      "                               ...                        \n",
      "11309    nyeda cnsvax uwec edu david nye neurolog consu...\n",
      "11310    old mac mac plu problem screen blank sometim m...\n",
      "11311    newsread tin version instal cpu clone motherbo...\n",
      "11312    articl qkgbuinnsn shelley washington edu bolso...\n",
      "11313    stolen pasadena blue white honda cbrrr califor...\n",
      "Name: data_stem, Length: 11314, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "# Initialize PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define a function to apply stemming to a text\n",
    "def stem_text(text):\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Apply stemming to the 'data' column and store the result as 'data_stem'\n",
    "df['data_stem'] = df['data'].apply(lambda x: stem_text(x))\n",
    "\n",
    "print(df['data_stem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stem = df['data_stem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        wonder anyon could enlighten car saw day door ...\n",
       "1        fair number brave soul upgrad clock oscil shar...\n",
       "2        well folk mac plu final gave ghost weekend sta...\n",
       "3        newsread tin version robert kyanko rob rjck uu...\n",
       "4        articl cowcb world std com tombak world std co...\n",
       "                               ...                        \n",
       "11309    nyeda cnsvax uwec edu david nye neurolog consu...\n",
       "11310    old mac mac plu problem screen blank sometim m...\n",
       "11311    newsread tin version instal cpu clone motherbo...\n",
       "11312    articl qkgbuinnsn shelley washington edu bolso...\n",
       "11313    stolen pasadena blue white honda cbrrr califor...\n",
       "Name: data_stem, Length: 11314, dtype: object"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_stem = stem_text(df['data'])\n",
    "\n",
    "#print(data_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part E 11 --> Here we applied data_stem which applies stemming to words in the text, reducing them to the base or root form. (eg. running becomes run 'fishing' becomes fish and so on). In other words stemming normalizes words by converting different nfected or derived forms of a word to a common base form. This reduces vocabulary size and improves the efficiency of text processing algorithms.\n",
    "\n",
    "Other benefits: Improved model generalization, because it treats different forms of words as the same word and therefore capturing the underlying semantics more effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Document 1: w\n",
      "Lemmatized Document 2: o\n",
      "Lemmatized Document 3: n\n",
      "Lemmatized Document 4: d\n",
      "Lemmatized Document 5: e\n",
      "Lemmatized Document 6: r\n",
      "Lemmatized Document 7: I\n",
      "Lemmatized Document 8: n\n",
      "Lemmatized Document 9: g\n",
      "Lemmatized Document 10:  \n",
      "Lemmatized Document 11: a\n",
      "Lemmatized Document 12: n\n",
      "Lemmatized Document 13: y\n",
      "Lemmatized Document 14: o\n",
      "Lemmatized Document 15: n\n",
      "Lemmatized Document 16: e\n",
      "Lemmatized Document 17:  \n",
      "Lemmatized Document 18: c\n",
      "Lemmatized Document 19: o\n",
      "Lemmatized Document 20: u\n",
      "Lemmatized Document 21: l\n",
      "Lemmatized Document 22: d\n",
      "Lemmatized Document 23:  \n",
      "Lemmatized Document 24: e\n",
      "Lemmatized Document 25: n\n",
      "Lemmatized Document 26: l\n",
      "Lemmatized Document 27: I\n",
      "Lemmatized Document 28: g\n",
      "Lemmatized Document 29: h\n",
      "Lemmatized Document 30: t\n",
      "Lemmatized Document 31: e\n",
      "Lemmatized Document 32: n\n",
      "Lemmatized Document 33:  \n",
      "Lemmatized Document 34: c\n",
      "Lemmatized Document 35: a\n",
      "Lemmatized Document 36: r\n",
      "Lemmatized Document 37:  \n",
      "Lemmatized Document 38: s\n",
      "Lemmatized Document 39: a\n",
      "Lemmatized Document 40: w\n",
      "Lemmatized Document 41:  \n",
      "Lemmatized Document 42: d\n",
      "Lemmatized Document 43: a\n",
      "Lemmatized Document 44: y\n",
      "Lemmatized Document 45:  \n",
      "Lemmatized Document 46: d\n",
      "Lemmatized Document 47: o\n",
      "Lemmatized Document 48: o\n",
      "Lemmatized Document 49: r\n",
      "Lemmatized Document 50:  \n",
      "Lemmatized Document 51: s\n",
      "Lemmatized Document 52: p\n",
      "Lemmatized Document 53: o\n",
      "Lemmatized Document 54: r\n",
      "Lemmatized Document 55: t\n",
      "Lemmatized Document 56: s\n",
      "Lemmatized Document 57:  \n",
      "Lemmatized Document 58: c\n",
      "Lemmatized Document 59: a\n",
      "Lemmatized Document 60: r\n",
      "Lemmatized Document 61:  \n",
      "Lemmatized Document 62: l\n",
      "Lemmatized Document 63: o\n",
      "Lemmatized Document 64: o\n",
      "Lemmatized Document 65: k\n",
      "Lemmatized Document 66: e\n",
      "Lemmatized Document 67: d\n",
      "Lemmatized Document 68:  \n",
      "Lemmatized Document 69: l\n",
      "Lemmatized Document 70: a\n",
      "Lemmatized Document 71: t\n",
      "Lemmatized Document 72: e\n",
      "Lemmatized Document 73:  \n",
      "Lemmatized Document 74: e\n",
      "Lemmatized Document 75: a\n",
      "Lemmatized Document 76: r\n",
      "Lemmatized Document 77: l\n",
      "Lemmatized Document 78: y\n",
      "Lemmatized Document 79:  \n",
      "Lemmatized Document 80: c\n",
      "Lemmatized Document 81: a\n",
      "Lemmatized Document 82: l\n",
      "Lemmatized Document 83: l\n",
      "Lemmatized Document 84: e\n",
      "Lemmatized Document 85: d\n",
      "Lemmatized Document 86:  \n",
      "Lemmatized Document 87: b\n",
      "Lemmatized Document 88: r\n",
      "Lemmatized Document 89: I\n",
      "Lemmatized Document 90: c\n",
      "Lemmatized Document 91: k\n",
      "Lemmatized Document 92: l\n",
      "Lemmatized Document 93: I\n",
      "Lemmatized Document 94: n\n",
      "Lemmatized Document 95:  \n",
      "Lemmatized Document 96: d\n",
      "Lemmatized Document 97: o\n",
      "Lemmatized Document 98: o\n",
      "Lemmatized Document 99: r\n",
      "Lemmatized Document 100: s\n",
      "Lemmatized Document 101:  \n",
      "Lemmatized Document 102: r\n",
      "Lemmatized Document 103: e\n",
      "Lemmatized Document 104: a\n",
      "Lemmatized Document 105: l\n",
      "Lemmatized Document 106: l\n",
      "Lemmatized Document 107: y\n",
      "Lemmatized Document 108:  \n",
      "Lemmatized Document 109: s\n",
      "Lemmatized Document 110: m\n",
      "Lemmatized Document 111: a\n",
      "Lemmatized Document 112: l\n",
      "Lemmatized Document 113: l\n",
      "Lemmatized Document 114:  \n",
      "Lemmatized Document 115: a\n",
      "Lemmatized Document 116: d\n",
      "Lemmatized Document 117: d\n",
      "Lemmatized Document 118: I\n",
      "Lemmatized Document 119: t\n",
      "Lemmatized Document 120: I\n",
      "Lemmatized Document 121: o\n",
      "Lemmatized Document 122: n\n",
      "Lemmatized Document 123:  \n",
      "Lemmatized Document 124: f\n",
      "Lemmatized Document 125: r\n",
      "Lemmatized Document 126: o\n",
      "Lemmatized Document 127: n\n",
      "Lemmatized Document 128: t\n",
      "Lemmatized Document 129:  \n",
      "Lemmatized Document 130: b\n",
      "Lemmatized Document 131: u\n",
      "Lemmatized Document 132: m\n",
      "Lemmatized Document 133: p\n",
      "Lemmatized Document 134: e\n",
      "Lemmatized Document 135: r\n",
      "Lemmatized Document 136:  \n",
      "Lemmatized Document 137: s\n",
      "Lemmatized Document 138: e\n",
      "Lemmatized Document 139: p\n",
      "Lemmatized Document 140: a\n",
      "Lemmatized Document 141: r\n",
      "Lemmatized Document 142: a\n",
      "Lemmatized Document 143: t\n",
      "Lemmatized Document 144: e\n",
      "Lemmatized Document 145:  \n",
      "Lemmatized Document 146: r\n",
      "Lemmatized Document 147: e\n",
      "Lemmatized Document 148: s\n",
      "Lemmatized Document 149: t\n",
      "Lemmatized Document 150:  \n",
      "Lemmatized Document 151: b\n",
      "Lemmatized Document 152: o\n",
      "Lemmatized Document 153: d\n",
      "Lemmatized Document 154: y\n",
      "Lemmatized Document 155:  \n",
      "Lemmatized Document 156: k\n",
      "Lemmatized Document 157: n\n",
      "Lemmatized Document 158: o\n",
      "Lemmatized Document 159: w\n",
      "Lemmatized Document 160:  \n",
      "Lemmatized Document 161: a\n",
      "Lemmatized Document 162: n\n",
      "Lemmatized Document 163: y\n",
      "Lemmatized Document 164: o\n",
      "Lemmatized Document 165: n\n",
      "Lemmatized Document 166: e\n",
      "Lemmatized Document 167:  \n",
      "Lemmatized Document 168: t\n",
      "Lemmatized Document 169: e\n",
      "Lemmatized Document 170: l\n",
      "Lemmatized Document 171: l\n",
      "Lemmatized Document 172: m\n",
      "Lemmatized Document 173: e\n",
      "Lemmatized Document 174:  \n",
      "Lemmatized Document 175: m\n",
      "Lemmatized Document 176: o\n",
      "Lemmatized Document 177: d\n",
      "Lemmatized Document 178: e\n",
      "Lemmatized Document 179: l\n",
      "Lemmatized Document 180:  \n",
      "Lemmatized Document 181: n\n",
      "Lemmatized Document 182: a\n",
      "Lemmatized Document 183: m\n",
      "Lemmatized Document 184: e\n",
      "Lemmatized Document 185:  \n",
      "Lemmatized Document 186: e\n",
      "Lemmatized Document 187: n\n",
      "Lemmatized Document 188: g\n",
      "Lemmatized Document 189: I\n",
      "Lemmatized Document 190: n\n",
      "Lemmatized Document 191: e\n",
      "Lemmatized Document 192:  \n",
      "Lemmatized Document 193: s\n",
      "Lemmatized Document 194: p\n",
      "Lemmatized Document 195: e\n",
      "Lemmatized Document 196: c\n",
      "Lemmatized Document 197: s\n",
      "Lemmatized Document 198:  \n",
      "Lemmatized Document 199: y\n",
      "Lemmatized Document 200: e\n",
      "Lemmatized Document 201: a\n",
      "Lemmatized Document 202: r\n",
      "Lemmatized Document 203: s\n",
      "Lemmatized Document 204:  \n",
      "Lemmatized Document 205: p\n",
      "Lemmatized Document 206: r\n",
      "Lemmatized Document 207: o\n",
      "Lemmatized Document 208: d\n",
      "Lemmatized Document 209: u\n",
      "Lemmatized Document 210: c\n",
      "Lemmatized Document 211: t\n",
      "Lemmatized Document 212: I\n",
      "Lemmatized Document 213: o\n",
      "Lemmatized Document 214: n\n",
      "Lemmatized Document 215:  \n",
      "Lemmatized Document 216: c\n",
      "Lemmatized Document 217: a\n",
      "Lemmatized Document 218: r\n",
      "Lemmatized Document 219:  \n",
      "Lemmatized Document 220: m\n",
      "Lemmatized Document 221: a\n",
      "Lemmatized Document 222: d\n",
      "Lemmatized Document 223: e\n",
      "Lemmatized Document 224:  \n",
      "Lemmatized Document 225: h\n",
      "Lemmatized Document 226: I\n",
      "Lemmatized Document 227: s\n",
      "Lemmatized Document 228: t\n",
      "Lemmatized Document 229: o\n",
      "Lemmatized Document 230: r\n",
      "Lemmatized Document 231: y\n",
      "Lemmatized Document 232:  \n",
      "Lemmatized Document 233: w\n",
      "Lemmatized Document 234: h\n",
      "Lemmatized Document 235: a\n",
      "Lemmatized Document 236: t\n",
      "Lemmatized Document 237: e\n",
      "Lemmatized Document 238: v\n",
      "Lemmatized Document 239: e\n",
      "Lemmatized Document 240: r\n",
      "Lemmatized Document 241:  \n",
      "Lemmatized Document 242: I\n",
      "Lemmatized Document 243: n\n",
      "Lemmatized Document 244: f\n",
      "Lemmatized Document 245: o\n",
      "Lemmatized Document 246:  \n",
      "Lemmatized Document 247: f\n",
      "Lemmatized Document 248: u\n",
      "Lemmatized Document 249: n\n",
      "Lemmatized Document 250: k\n",
      "Lemmatized Document 251: y\n",
      "Lemmatized Document 252:  \n",
      "Lemmatized Document 253: l\n",
      "Lemmatized Document 254: o\n",
      "Lemmatized Document 255: o\n",
      "Lemmatized Document 256: k\n",
      "Lemmatized Document 257: I\n",
      "Lemmatized Document 258: n\n",
      "Lemmatized Document 259: g\n",
      "Lemmatized Document 260:  \n",
      "Lemmatized Document 261: c\n",
      "Lemmatized Document 262: a\n",
      "Lemmatized Document 263: r\n",
      "Lemmatized Document 264:  \n",
      "Lemmatized Document 265: p\n",
      "Lemmatized Document 266: l\n",
      "Lemmatized Document 267: e\n",
      "Lemmatized Document 268: a\n",
      "Lemmatized Document 269: s\n",
      "Lemmatized Document 270: e\n",
      "Lemmatized Document 271:  \n",
      "Lemmatized Document 272: m\n",
      "Lemmatized Document 273: a\n",
      "Lemmatized Document 274: I\n",
      "Lemmatized Document 275: l\n",
      "Lemmatized Document 276:  \n",
      "Lemmatized Document 277: t\n",
      "Lemmatized Document 278: h\n",
      "Lemmatized Document 279: a\n",
      "Lemmatized Document 280: n\n",
      "Lemmatized Document 281: k\n",
      "Lemmatized Document 282: s\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# Initialize Spacy 'en' model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Data to be lemmatized\n",
    "data = df['data'][0]\n",
    "\n",
    "# Initialize the list to store lemmatized documents\n",
    "data_lem = []\n",
    "\n",
    "# Apply lemmatization using Spacy\n",
    "for doc in data:\n",
    "    # Process the document using Spacy\n",
    "    doc_spacy = nlp(doc)\n",
    "    \n",
    "    # Extract lemmas for each token and join them back\n",
    "    lemmas = [token.lemma_ for token in doc_spacy]\n",
    "    doc_lem = \" \".join(lemmas)\n",
    "    \n",
    "    # Append the lemmatized document to data_lem list\n",
    "    data_lem.append(doc_lem)\n",
    "\n",
    "# Print the lemmatized documents\n",
    "for idx, doc in enumerate(data_lem):\n",
    "    print(f\"Lemmatized Document {idx + 1}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 30522\n",
      "['of', 'and', 'in', 'to', 'was', 'he', 'is', 'as', 'for', 'on', 'with', 'that', 'it', 'his', 'by', 'at', 'from', 'her', '##s', 'she', 'you', 'had', 'an', 'were', 'but', 'be', 'this', 'are', 'not', 'my', 'they', 'one', 'which', 'or', 'have', 'him', 'me', 'first', 'all', 'also', 'their', 'has', 'up', 'who', 'out', 'been', 'when', 'after', 'there', 'into', 'new', 'two', 'its', '##a', 'time', 'would', 'no', 'what', 'about', 'said', 'we', 'over', 'then', 'other', 'so', 'more', '##e', 'can', 'if', 'like', 'back', 'them', 'only', 'some', 'could', '##i', 'where', 'just', '##ing', 'during', 'before', '##n', 'do', '##o', 'made', 'school', 'through', 'than', 'now', 'years', 'most', 'world', 'may', 'between', 'down', 'well', 'three', '##d', 'year', 'while', 'will', '##ed', '##r']\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer\n",
    "bert_uncased = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Print the vocabulary size\n",
    "print(\"Vocabulary size:\", len(bert_uncased.vocab))\n",
    "\n",
    "# Print some tokens from the vocabulary\n",
    "print(list(bert_uncased.vocab.keys())[1997:2100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part E 13.a.: What is the vocabulary size? It's 30522 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Document 0: w\n"
     ]
    }
   ],
   "source": [
    "# This is our data\n",
    "data = df['data'][0]\n",
    "\n",
    "# Initialize the list to store BERT-tokenized data\n",
    "data_BERT = []\n",
    "\n",
    "# Apply BERT-tokenization to each text\n",
    "for doc in data:\n",
    "    # Tokenize the text using BERT tokenizer\n",
    "    tokens = bert_uncased.tokenize(doc)\n",
    "    \n",
    "    # Join tokens back to form a string\n",
    "    tokenized_text = \" \".join(tokens)\n",
    "    \n",
    "    # Append the tokenized text to data_BERT list\n",
    "    data_BERT.append(tokenized_text)\n",
    "\n",
    "# Print the first tokenized document\n",
    "print(\"Tokenized Document 0:\", data_BERT[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'w'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_BERT[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wondering anyone could enlighten car saw day door sports car looked late early called bricklin doors really small addition front bumper separate rest body know anyone tellme model name engine specs years production car made history whatever info funky looking car please mail thanks'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w', 'o', 'n', 'd', 'e', 'r', 'I', 'n', 'g', ' ', 'a', 'n', 'y', 'o', 'n', 'e', ' ', 'c', 'o', 'u', 'l', 'd', ' ', 'e', 'n', 'l', 'I', 'g', 'h', 't', 'e', 'n', ' ', 'c', 'a', 'r', ' ', 's', 'a', 'w', ' ', 'd', 'a', 'y', ' ', 'd', 'o', 'o', 'r', ' ', 's', 'p', 'o', 'r', 't', 's', ' ', 'c', 'a', 'r', ' ', 'l', 'o', 'o', 'k', 'e', 'd', ' ', 'l', 'a', 't', 'e', ' ', 'e', 'a', 'r', 'l', 'y', ' ', 'c', 'a', 'l', 'l', 'e', 'd', ' ', 'b', 'r', 'I', 'c', 'k', 'l', 'I', 'n', ' ', 'd', 'o', 'o', 'r', 's', ' ', 'r', 'e', 'a', 'l', 'l', 'y', ' ', 's', 'm', 'a', 'l', 'l', ' ', 'a', 'd', 'd', 'I', 't', 'I', 'o', 'n', ' ', 'f', 'r', 'o', 'n', 't', ' ', 'b', 'u', 'm', 'p', 'e', 'r', ' ', 's', 'e', 'p', 'a', 'r', 'a', 't', 'e', ' ', 'r', 'e', 's', 't', ' ', 'b', 'o', 'd', 'y', ' ', 'k', 'n', 'o', 'w', ' ', 'a', 'n', 'y', 'o', 'n', 'e', ' ', 't', 'e', 'l', 'l', 'm', 'e', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'n', 'a', 'm', 'e', ' ', 'e', 'n', 'g', 'I', 'n', 'e', ' ', 's', 'p', 'e', 'c', 's', ' ', 'y', 'e', 'a', 'r', 's', ' ', 'p', 'r', 'o', 'd', 'u', 'c', 't', 'I', 'o', 'n', ' ', 'c', 'a', 'r', ' ', 'm', 'a', 'd', 'e', ' ', 'h', 'I', 's', 't', 'o', 'r', 'y', ' ', 'w', 'h', 'a', 't', 'e', 'v', 'e', 'r', ' ', 'I', 'n', 'f', 'o', ' ', 'f', 'u', 'n', 'k', 'y', ' ', 'l', 'o', 'o', 'k', 'I', 'n', 'g', ' ', 'c', 'a', 'r', ' ', 'p', 'l', 'e', 'a', 's', 'e', ' ', 'm', 'a', 'I', 'l', ' ', 't', 'h', 'a', 'n', 'k', 's']\n"
     ]
    }
   ],
   "source": [
    "print(data_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        wonder anyon could enlighten car saw day door ...\n",
      "1        fair number brave soul upgrad clock oscil shar...\n",
      "2        well folk mac plu final gave ghost weekend sta...\n",
      "3        newsread tin version robert kyanko rob rjck uu...\n",
      "4        articl cowcb world std com tombak world std co...\n",
      "                               ...                        \n",
      "11309    nyeda cnsvax uwec edu david nye neurolog consu...\n",
      "11310    old mac mac plu problem screen blank sometim m...\n",
      "11311    newsread tin version instal cpu clone motherbo...\n",
      "11312    articl qkgbuinnsn shelley washington edu bolso...\n",
      "11313    stolen pasadena blue white honda cbrrr califor...\n",
      "Name: data_stem, Length: 11314, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wondering anyone could enlighten car saw day door sports car looked late early called bricklin doors really small addition front bumper separate rest body know anyone tellme model name engine specs years production car made history whatever info funky looking car please mail thanks\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        wonder anyon could enlighten car saw day door ...\n",
       "1        fair number brave soul upgrad clock oscil shar...\n",
       "2        well folk mac plu final gave ghost weekend sta...\n",
       "3        newsread tin version robert kyanko rob rjck uu...\n",
       "4        articl cowcb world std com tombak world std co...\n",
       "                               ...                        \n",
       "11309    nyeda cnsvax uwec edu david nye neurolog consu...\n",
       "11310    old mac mac plu problem screen blank sometim m...\n",
       "11311    newsread tin version instal cpu clone motherbo...\n",
       "11312    articl qkgbuinnsn shelley washington edu bolso...\n",
       "11313    stolen pasadena blue white honda cbrrr califor...\n",
       "Name: data_stem, Length: 11314, dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wondering anyone could enlighten car saw day door sports car looked late early called bricklin doors really small addition front bumper separate rest body know anyone tellme model name engine specs years production car made history whatever info funky looking car please mail thanks'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Bag of Words (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in CountVectorizer: 300\n",
      "First entry in CountVectorizer features: ['abl' 'accept' 'actual' 'address' 'advanc' 'ago' 'agre' 'allow' 'alreadi'\n",
      " 'also' 'alway' 'american' 'anoth' 'answer' 'anyon' 'anyth' 'anyway'\n",
      " 'appreci' 'apr' 'area' 'around' 'articl' 'ask' 'assum' 'avail' 'away'\n",
      " 'back' 'bad' 'base' 'becom' 'believ' 'best' 'better' 'big' 'bit' 'book'\n",
      " 'buy' 'call' 'cannot' 'car' 'card' 'care' 'case' 'caus' 'chang' 'check'\n",
      " 'christian' 'claim' 'close' 'com' 'come' 'complet' 'comput' 'consid'\n",
      " 'control' 'correct' 'cost' 'could' 'cours' 'creat' 'current' 'data'\n",
      " 'david' 'day' 'design' 'differ' 'discuss' 'done' 'drive' 'edu' 'effect'\n",
      " 'either' 'els' 'email' 'end' 'engin' 'enough' 'etc' 'even' 'ever' 'everi'\n",
      " 'exampl' 'except' 'exist' 'expect' 'experi' 'fact' 'far' 'fax' 'feel'\n",
      " 'file' 'find' 'first' 'follow' 'forc' 'found' 'free' 'full' 'game'\n",
      " 'gener' 'get' 'give' 'given' 'go' 'god' 'good' 'got' 'govern' 'great'\n",
      " 'group' 'guess' 'hand' 'happen' 'hard' 'heard' 'help' 'high' 'home'\n",
      " 'hope' 'howev' 'human' 'idea' 'import' 'includ' 'inform' 'interest'\n",
      " 'internet' 'isn' 'issu' 'john' 'keep' 'key' 'kill' 'kind' 'know' 'larg'\n",
      " 'last' 'law' 'least' 'left' 'less' 'let' 'life' 'like' 'line' 'list'\n",
      " 'littl' 'live' 'long' 'look' 'lot' 'machin' 'made' 'mail' 'major' 'make'\n",
      " 'man' 'mani' 'mark' 'matter' 'may' 'mayb' 'mean' 'mention' 'messag'\n",
      " 'might' 'mind' 'move' 'much' 'must' 'name' 'nation' 'need' 'net' 'never'\n",
      " 'new' 'news' 'newsread' 'next' 'non' 'not' 'note' 'noth' 'number' 'old'\n",
      " 'one' 'open' 'opinion' 'order' 'origin' 'other' 'part' 'peopl' 'perhap'\n",
      " 'person' 'phone' 'place' 'play' 'pleas' 'point' 'possibl' 'post' 'power'\n",
      " 'pretti' 'price' 'probabl' 'problem' 'program' 'provid' 'public' 'put'\n",
      " 'question' 'quit' 'rather' 'read' 'real' 'realli' 'reason' 'recent'\n",
      " 'refer' 'rememb' 'repli' 'report' 'requir' 'research' 'respons' 'result'\n",
      " 'right' 'run' 'said' 'say' 'second' 'see' 'seem' 'seen' 'send' 'set'\n",
      " 'sever' 'show' 'side' 'sinc' 'small' 'softwar' 'someon' 'someth' 'sound'\n",
      " 'sourc' 'space' 'standard' 'start' 'state' 'still' 'subject' 'suggest'\n",
      " 'support' 'sure' 'system' 'take' 'talk' 'team' 'tell' 'thank' 'thing'\n",
      " 'think' 'though' 'thought' 'three' 'time' 'today' 'tri' 'true' 'turn'\n",
      " 'two' 'type' 'understand' 'univers' 'use' 'usual' 'version' 'view' 'want'\n",
      " 'way' 'week' 'well' 'whether' 'whole' 'win' 'window' 'without' 'wonder'\n",
      " 'word' 'work' 'world' 'would' 'write' 'wrong' 'wrote' 'ye' 'year' 'yet']\n"
     ]
    }
   ],
   "source": [
    "# Initialize CountVectorizer\n",
    "count_vectorizer = CountVectorizer(max_df=0.95, min_df=0.05)\n",
    "\n",
    "# Fit and transform the documents using CountVectorizer\n",
    "count_matrix = count_vectorizer.fit_transform(data_stem)\n",
    "\n",
    "# Get feature names\n",
    "count_feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print number of features and first entry\n",
    "print(\"Number of features in CountVectorizer:\", len(count_feature_names))\n",
    "print(\"First entry in CountVectorizer features:\", count_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " CountVectorizer converts a collection of text documents into a matrix of token counts. Here, max_df and min_df are used to filter out terms based on their document frequency in the corpus. Terms that occur in more than 95% of the documents (max_df=0.95) or less than 5% of the documents (min_df=0.05) are ignored.\n",
    "\n",
    " Pro: Simple to understand & implement. It captures the frequency of terms in each document accurately\n",
    " Con: Doesn'T consider importance of terms in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in TfidfVectorizer (L1): 300\n",
      "First entry in TfidfVectorizer (L1) features: ['abl' 'accept' 'actual' 'address' 'advanc' 'ago' 'agre' 'allow' 'alreadi'\n",
      " 'also' 'alway' 'american' 'anoth' 'answer' 'anyon' 'anyth' 'anyway'\n",
      " 'appreci' 'apr' 'area' 'around' 'articl' 'ask' 'assum' 'avail' 'away'\n",
      " 'back' 'bad' 'base' 'becom' 'believ' 'best' 'better' 'big' 'bit' 'book'\n",
      " 'buy' 'call' 'cannot' 'car' 'card' 'care' 'case' 'caus' 'chang' 'check'\n",
      " 'christian' 'claim' 'close' 'com' 'come' 'complet' 'comput' 'consid'\n",
      " 'control' 'correct' 'cost' 'could' 'cours' 'creat' 'current' 'data'\n",
      " 'david' 'day' 'design' 'differ' 'discuss' 'done' 'drive' 'edu' 'effect'\n",
      " 'either' 'els' 'email' 'end' 'engin' 'enough' 'etc' 'even' 'ever' 'everi'\n",
      " 'exampl' 'except' 'exist' 'expect' 'experi' 'fact' 'far' 'fax' 'feel'\n",
      " 'file' 'find' 'first' 'follow' 'forc' 'found' 'free' 'full' 'game'\n",
      " 'gener' 'get' 'give' 'given' 'go' 'god' 'good' 'got' 'govern' 'great'\n",
      " 'group' 'guess' 'hand' 'happen' 'hard' 'heard' 'help' 'high' 'home'\n",
      " 'hope' 'howev' 'human' 'idea' 'import' 'includ' 'inform' 'interest'\n",
      " 'internet' 'isn' 'issu' 'john' 'keep' 'key' 'kill' 'kind' 'know' 'larg'\n",
      " 'last' 'law' 'least' 'left' 'less' 'let' 'life' 'like' 'line' 'list'\n",
      " 'littl' 'live' 'long' 'look' 'lot' 'machin' 'made' 'mail' 'major' 'make'\n",
      " 'man' 'mani' 'mark' 'matter' 'may' 'mayb' 'mean' 'mention' 'messag'\n",
      " 'might' 'mind' 'move' 'much' 'must' 'name' 'nation' 'need' 'net' 'never'\n",
      " 'new' 'news' 'newsread' 'next' 'non' 'not' 'note' 'noth' 'number' 'old'\n",
      " 'one' 'open' 'opinion' 'order' 'origin' 'other' 'part' 'peopl' 'perhap'\n",
      " 'person' 'phone' 'place' 'play' 'pleas' 'point' 'possibl' 'post' 'power'\n",
      " 'pretti' 'price' 'probabl' 'problem' 'program' 'provid' 'public' 'put'\n",
      " 'question' 'quit' 'rather' 'read' 'real' 'realli' 'reason' 'recent'\n",
      " 'refer' 'rememb' 'repli' 'report' 'requir' 'research' 'respons' 'result'\n",
      " 'right' 'run' 'said' 'say' 'second' 'see' 'seem' 'seen' 'send' 'set'\n",
      " 'sever' 'show' 'side' 'sinc' 'small' 'softwar' 'someon' 'someth' 'sound'\n",
      " 'sourc' 'space' 'standard' 'start' 'state' 'still' 'subject' 'suggest'\n",
      " 'support' 'sure' 'system' 'take' 'talk' 'team' 'tell' 'thank' 'thing'\n",
      " 'think' 'though' 'thought' 'three' 'time' 'today' 'tri' 'true' 'turn'\n",
      " 'two' 'type' 'understand' 'univers' 'use' 'usual' 'version' 'view' 'want'\n",
      " 'way' 'week' 'well' 'whether' 'whole' 'win' 'window' 'without' 'wonder'\n",
      " 'word' 'work' 'world' 'would' 'write' 'wrong' 'wrote' 'ye' 'year' 'yet']\n"
     ]
    }
   ],
   "source": [
    "# Initialize TfidfVectorizer without IDF and with L1 normalization\n",
    "tfidf_vectorizer_l1 = TfidfVectorizer(max_df=0.95, min_df=0.05, use_idf=False, norm='l1')\n",
    "\n",
    "# Fit and transform the documents using TfidfVectorizer with L1 normalization\n",
    "tfidf_matrix_l1 = tfidf_vectorizer_l1.fit_transform(data_stem)\n",
    "\n",
    "# Get feature names\n",
    "tfidf_feature_names_l1 = tfidf_vectorizer_l1.get_feature_names_out()\n",
    "\n",
    "# Print number of features and first entry\n",
    "print(\"Number of features in TfidfVectorizer (L1):\", len(tfidf_feature_names_l1))\n",
    "print(\"First entry in TfidfVectorizer (L1) features:\", tfidf_feature_names_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer transforms text into a matrix of TF-IDF features. Here, use_idf=False means we are not using Inverse Document Frequency (IDF) weighting, and norm='l1' normalizes each output row to have unit L1 norm.\n",
    "\n",
    "Pro: Captures term frequency while accounting for document length differences (with L1 noralization)\n",
    "\n",
    "Con: Doesn't consider IDF, which could ffect the importance of rare terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in TfidfVectorizer (no smoothing): 300\n",
      "First entry in TfidfVectorizer (no smoothing) features: ['abl' 'accept' 'actual' 'address' 'advanc' 'ago' 'agre' 'allow' 'alreadi'\n",
      " 'also' 'alway' 'american' 'anoth' 'answer' 'anyon' 'anyth' 'anyway'\n",
      " 'appreci' 'apr' 'area' 'around' 'articl' 'ask' 'assum' 'avail' 'away'\n",
      " 'back' 'bad' 'base' 'becom' 'believ' 'best' 'better' 'big' 'bit' 'book'\n",
      " 'buy' 'call' 'cannot' 'car' 'card' 'care' 'case' 'caus' 'chang' 'check'\n",
      " 'christian' 'claim' 'close' 'com' 'come' 'complet' 'comput' 'consid'\n",
      " 'control' 'correct' 'cost' 'could' 'cours' 'creat' 'current' 'data'\n",
      " 'david' 'day' 'design' 'differ' 'discuss' 'done' 'drive' 'edu' 'effect'\n",
      " 'either' 'els' 'email' 'end' 'engin' 'enough' 'etc' 'even' 'ever' 'everi'\n",
      " 'exampl' 'except' 'exist' 'expect' 'experi' 'fact' 'far' 'fax' 'feel'\n",
      " 'file' 'find' 'first' 'follow' 'forc' 'found' 'free' 'full' 'game'\n",
      " 'gener' 'get' 'give' 'given' 'go' 'god' 'good' 'got' 'govern' 'great'\n",
      " 'group' 'guess' 'hand' 'happen' 'hard' 'heard' 'help' 'high' 'home'\n",
      " 'hope' 'howev' 'human' 'idea' 'import' 'includ' 'inform' 'interest'\n",
      " 'internet' 'isn' 'issu' 'john' 'keep' 'key' 'kill' 'kind' 'know' 'larg'\n",
      " 'last' 'law' 'least' 'left' 'less' 'let' 'life' 'like' 'line' 'list'\n",
      " 'littl' 'live' 'long' 'look' 'lot' 'machin' 'made' 'mail' 'major' 'make'\n",
      " 'man' 'mani' 'mark' 'matter' 'may' 'mayb' 'mean' 'mention' 'messag'\n",
      " 'might' 'mind' 'move' 'much' 'must' 'name' 'nation' 'need' 'net' 'never'\n",
      " 'new' 'news' 'newsread' 'next' 'non' 'not' 'note' 'noth' 'number' 'old'\n",
      " 'one' 'open' 'opinion' 'order' 'origin' 'other' 'part' 'peopl' 'perhap'\n",
      " 'person' 'phone' 'place' 'play' 'pleas' 'point' 'possibl' 'post' 'power'\n",
      " 'pretti' 'price' 'probabl' 'problem' 'program' 'provid' 'public' 'put'\n",
      " 'question' 'quit' 'rather' 'read' 'real' 'realli' 'reason' 'recent'\n",
      " 'refer' 'rememb' 'repli' 'report' 'requir' 'research' 'respons' 'result'\n",
      " 'right' 'run' 'said' 'say' 'second' 'see' 'seem' 'seen' 'send' 'set'\n",
      " 'sever' 'show' 'side' 'sinc' 'small' 'softwar' 'someon' 'someth' 'sound'\n",
      " 'sourc' 'space' 'standard' 'start' 'state' 'still' 'subject' 'suggest'\n",
      " 'support' 'sure' 'system' 'take' 'talk' 'team' 'tell' 'thank' 'thing'\n",
      " 'think' 'though' 'thought' 'three' 'time' 'today' 'tri' 'true' 'turn'\n",
      " 'two' 'type' 'understand' 'univers' 'use' 'usual' 'version' 'view' 'want'\n",
      " 'way' 'week' 'well' 'whether' 'whole' 'win' 'window' 'without' 'wonder'\n",
      " 'word' 'work' 'world' 'would' 'write' 'wrong' 'wrote' 'ye' 'year' 'yet']\n"
     ]
    }
   ],
   "source": [
    "# Initialize TfidfVectorizer without IDF smoothing\n",
    "tfidf_vectorizer_no_smooth = TfidfVectorizer(max_df=0.95, min_df=0.05, smooth_idf=False)\n",
    "\n",
    "# Fit and transform the documents using TfidfVectorizer without IDF smoothing\n",
    "tfidf_matrix_no_smooth = tfidf_vectorizer_no_smooth.fit_transform(data_stem)\n",
    "\n",
    "# Get feature names\n",
    "tfidf_feature_names_no_smooth = tfidf_vectorizer_no_smooth.get_feature_names_out()\n",
    "\n",
    "# Print number of features and first entry\n",
    "print(\"Number of features in TfidfVectorizer (no smoothing):\", len(tfidf_feature_names_no_smooth))\n",
    "print(\"First entry in TfidfVectorizer (no smoothing) features:\", tfidf_feature_names_no_smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous TfidfVectorizer but with smooth_idf=False, which means we do not apply IDF smoothing to the weights.\n",
    "\n",
    "Pro: It provides TF-IDF weights without IDF smoothing, which could slightly affect the weights of rare terms.\n",
    "\n",
    "Con: Without IDF smoothing, rare terms might get overly emphasized in importance\n",
    "\n",
    "Features are all the same here (I think this is wrong). Changing max_df and min_df affects which terms are considered during vectorization. Lowering max_df includes more terms, while raising min_df excludes more terms from the feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Matrix:\n",
      "[[0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "Feature Names:\n",
      "['abl' 'accept' 'actual' 'address' 'advanc' 'ago' 'agre' 'allow' 'alreadi'\n",
      " 'also' 'alway' 'american' 'anoth' 'answer' 'anyon' 'anyth' 'anyway'\n",
      " 'appreci' 'apr' 'area' 'around' 'articl' 'ask' 'assum' 'avail' 'away'\n",
      " 'back' 'bad' 'base' 'becom' 'believ' 'best' 'better' 'big' 'bit' 'book'\n",
      " 'buy' 'call' 'cannot' 'car' 'card' 'care' 'case' 'caus' 'chang' 'check'\n",
      " 'christian' 'claim' 'close' 'com' 'come' 'complet' 'comput' 'consid'\n",
      " 'control' 'correct' 'cost' 'could' 'cours' 'creat' 'current' 'data'\n",
      " 'david' 'day' 'design' 'differ' 'discuss' 'done' 'drive' 'edu' 'effect'\n",
      " 'either' 'els' 'email' 'end' 'engin' 'enough' 'etc' 'even' 'ever' 'everi'\n",
      " 'exampl' 'except' 'exist' 'expect' 'experi' 'fact' 'far' 'fax' 'feel'\n",
      " 'file' 'find' 'first' 'follow' 'forc' 'found' 'free' 'full' 'game'\n",
      " 'gener' 'get' 'give' 'given' 'go' 'god' 'good' 'got' 'govern' 'great'\n",
      " 'group' 'guess' 'hand' 'happen' 'hard' 'heard' 'help' 'high' 'home'\n",
      " 'hope' 'howev' 'human' 'idea' 'import' 'includ' 'inform' 'interest'\n",
      " 'internet' 'isn' 'issu' 'john' 'keep' 'key' 'kill' 'kind' 'know' 'larg'\n",
      " 'last' 'law' 'least' 'left' 'less' 'let' 'life' 'like' 'line' 'list'\n",
      " 'littl' 'live' 'long' 'look' 'lot' 'machin' 'made' 'mail' 'major' 'make'\n",
      " 'man' 'mani' 'mark' 'matter' 'may' 'mayb' 'mean' 'mention' 'messag'\n",
      " 'might' 'mind' 'move' 'much' 'must' 'name' 'nation' 'need' 'net' 'never'\n",
      " 'new' 'news' 'newsread' 'next' 'non' 'not' 'note' 'noth' 'number' 'old'\n",
      " 'one' 'open' 'opinion' 'order' 'origin' 'other' 'part' 'peopl' 'perhap'\n",
      " 'person' 'phone' 'place' 'play' 'pleas' 'point' 'possibl' 'post' 'power'\n",
      " 'pretti' 'price' 'probabl' 'problem' 'program' 'provid' 'public' 'put'\n",
      " 'question' 'quit' 'rather' 'read' 'real' 'realli' 'reason' 'recent'\n",
      " 'refer' 'rememb' 'repli' 'report' 'requir' 'research' 'respons' 'result'\n",
      " 'right' 'run' 'said' 'say' 'second' 'see' 'seem' 'seen' 'send' 'set'\n",
      " 'sever' 'show' 'side' 'sinc' 'small' 'softwar' 'someon' 'someth' 'sound'\n",
      " 'sourc' 'space' 'standard' 'start' 'state' 'still' 'subject' 'suggest'\n",
      " 'support' 'sure' 'system' 'take' 'talk' 'team' 'tell' 'thank' 'thing'\n",
      " 'think' 'though' 'thought' 'three' 'time' 'today' 'tri' 'true' 'turn'\n",
      " 'two' 'type' 'understand' 'univers' 'use' 'usual' 'version' 'view' 'want'\n",
      " 'way' 'week' 'well' 'whether' 'whole' 'win' 'window' 'without' 'wonder'\n",
      " 'word' 'work' 'world' 'would' 'write' 'wrong' 'wrote' 'ye' 'year' 'yet']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Initialize Binarizer\n",
    "binarizer = Binarizer()\n",
    "\n",
    "# Apply Binarizer to the count matrix\n",
    "binary_matrix = binarizer.fit_transform(count_matrix)\n",
    "\n",
    "# Get feature names from CountVectorizer\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the binary matrix and feature names\n",
    "print(\"Binary Matrix:\")\n",
    "print(binary_matrix.toarray())\n",
    "print(\"\\nFeature Names:\")\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you apply Binarizer() to the result from a CountVectorizer, it converts the count matrix into a binary matrix where non-zero counts are transformed to 1, and zero counts remain as 0.\n",
    "\n",
    "Pros: \n",
    "\n",
    "Simplicity: The binary representation simplifies the data by removing the frequency information, which can sometimes be noisy or less relevant\n",
    "\n",
    "Ease of Use: Binary data is often easier to work with for certain machine learning algorithms, especially those that require binary input (e.g., association rule mining, certain types of clustering).\n",
    "\n",
    "\n",
    "Con: \n",
    "\n",
    "Loss of information: Binary representation loses the information about word frequency, which can be valuable in some natural language processing (NLP) tasks\n",
    "\n",
    "Lack of contect: It doesn't capture the degree of word importance or relevance within documents, which can be important for tasks like sentiment analysis, document classification, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted by TF-IDF Frequencies:\n",
      "       keys  Count_Frequency  TF_IDF_Frequency\n",
      "14    anyon                2                 1\n",
      "153    mail                1                 1\n",
      "289  wonder                1                 1\n",
      "261   thank                1                 1\n",
      "241   small                1                 1\n",
      "216  realli                1                 1\n",
      "198   pleas                1                 1\n",
      "170    name                1                 1\n",
      "152    made                1                 1\n",
      "37     call                1                 1\n",
      "149    look                2                 1\n",
      "134    know                1                 1\n",
      "75    engin                1                 1\n",
      "63      day                1                 1\n",
      "57    could                1                 1\n",
      "39      car                4                 1\n",
      "298    year                1                 1\n",
      "\n",
      "Sorted by Absolute Frequencies:\n",
      "       keys  Count_Frequency  TF_IDF_Frequency\n",
      "39      car                4                 1\n",
      "14    anyon                2                 1\n",
      "149    look                2                 1\n",
      "170    name                1                 1\n",
      "289  wonder                1                 1\n",
      "261   thank                1                 1\n",
      "241   small                1                 1\n",
      "216  realli                1                 1\n",
      "198   pleas                1                 1\n",
      "152    made                1                 1\n",
      "153    mail                1                 1\n",
      "37     call                1                 1\n",
      "134    know                1                 1\n",
      "75    engin                1                 1\n",
      "63      day                1                 1\n",
      "57    could                1                 1\n",
      "298    year                1                 1\n"
     ]
    }
   ],
   "source": [
    "# Convert the matrices to arrays\n",
    "count_array = count_matrix.toarray()\n",
    "binary_array = binary_matrix.toarray()\n",
    "\n",
    "# Extract frequencies for the first document\n",
    "first_doc_count = count_array[0]\n",
    "first_doc_tfidf = binary_array[0]\n",
    "\n",
    "# Create DataFrame with feature names, counts, and TF-IDF values for the first document\n",
    "data = {'keys': feature_names, 'Count_Frequency': first_doc_count, 'TF_IDF_Frequency': first_doc_tfidf}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Keep only rows for words existing in the first document\n",
    "df = df[df['Count_Frequency'] > 0]\n",
    "\n",
    "# Sort the DataFrame based on TF-IDF frequencies (descending)\n",
    "sorted_by_tfidf = df.sort_values(by='TF_IDF_Frequency', ascending=False)\n",
    "\n",
    "# Sort the DataFrame based on absolute frequencies (descending)\n",
    "sorted_by_count = df.sort_values(by='Count_Frequency', ascending=False)\n",
    "\n",
    "print(\"Sorted by TF-IDF Frequencies:\")\n",
    "print(sorted_by_tfidf)\n",
    "\n",
    "print(\"\\nSorted by Absolute Frequencies:\")\n",
    "print(sorted_by_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Bag of Words (gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_gen = [doc.split() for doc in data_stem]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command creates a corpus for Gensim using a list comprehension. Here's what each part does:\n",
    "\n",
    "for doc in data_stem: Iterates through each document in the data_stem variable.\n",
    "doc.split(): Splits each document into a list of words based on whitespace (assuming data_stem contains text strings).\n",
    "\n",
    "[...]: Wraps the list comprehension to create a list of lists, where each inner list represents a document broken into words.\n",
    "\n",
    "The corpus_gen variable now contains a list of lists, where each inner list represents a document's words. This format is suitable for building a Gensim corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create Dictionary from corpus_gen\n",
    "id2word = Dictionary(corpus_gen)\n",
    "\n",
    "# Filter extremes\n",
    "id2word.filter_extremes(no_below=566, no_above=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Gensim Dictionary from the corpus:\n",
    "\n",
    "id2word = Dictionary(corpus_gen): This line creates a Gensim Dictionary object (id2word) based on the corpus_gen we created earlier. The Dictionary object maps each unique word in the corpus to a unique integer ID.\n",
    "\n",
    "Apply filter_extremes method:\n",
    "id2word.filter_extremes(no_below=566, no_above=0.95): This method call filters out tokens (words) that are too rare or too common in the corpus.\n",
    "no_below=566: Specifies that tokens appearing in less than 566 documents will be removed. This helps remove very rare words that might not contribute much to the model.\n",
    "no_above=0.95: Specifies that tokens appearing in more than 95% of the documents will be removed. This helps remove very common words (like stopwords) that may not be informative.\n",
    "After applying filter_extremes, the id2word Dictionary object will contain only tokens that are neither too rare nor too common based on the specified thresholds. This cleaned dictionary is then used for further processing or modeling in Gensim."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
